{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"executionInfo":{"elapsed":23,"status":"ok","timestamp":1762852361487,"user":{"displayName":"Ayleen J.","userId":"09427723239281310820"},"user_tz":360},"id":"d0a_MUGoVs3m"},"outputs":[],"source":["# !pip install tensorflow --upgrade"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Xzy0VIRnpedV","executionInfo":{"status":"ok","timestamp":1762852381913,"user_tz":360,"elapsed":20421,"user":{"displayName":"Ayleen J.","userId":"09427723239281310820"}},"outputId":"84b34049-d7a6-421e-cf3f-903ea5c94dd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n","Collecting pip\n","  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n","Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.1.2\n","    Uninstalling pip-24.1.2:\n","      Successfully uninstalled pip-24.1.2\n","Successfully installed pip-25.3\n","Tue Nov 11 09:12:45 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n","| N/A   32C    P0             56W /  400W |       0MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-addons==0.23.0 (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-addons==0.23.0\u001b[0m\u001b[31m\n","\u001b[0mâœ… GPU detected: /device:GPU:0\n","TensorFlow version: 2.19.0\n","âœ… Ready with IMG_SIZE=224, BATCH_SIZE=16\n"]}],"source":["# ======================\n","# ğŸŒ¿ SKINTEREST - Setup\n","# ======================\n","\n","!python -m pip install --upgrade pip\n","!nvidia-smi\n","\n","# Install dependencies\n","!pip install -q kaggle==1.6.17 tensorflow==2.20.0 tensorflow-addons==0.23.0 opencv-python==4.10.0.84 \\\n","  pillow numpy pandas scikit-learn tqdm matplotlib seaborn streamlit\n","\n","# (Optional) Verify GPU type\n","import tensorflow as tf\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name:\n","    print(f\"âœ… GPU detected: {device_name}\")\n","else:\n","    print(\"âš ï¸ No GPU detected â€” go to Runtime > Change runtime type > select A100 or T4 GPU\")\n","\n","# Show TensorFlow version\n","print(\"TensorFlow version:\", tf.__version__)\n","\n","# (Optional) Check GPU memory\n","from tensorflow.python.client import device_lib\n","device_lib.list_local_devices()\n","\n","# ======================\n","# ğŸ’¾ Set Constants\n","# ======================\n","IMG_SIZE = 224  # consistent with project preprocessing\n","BATCH_SIZE = 16  # adjust if you hit OOM errors (try 8)\n","DATA_DIR = '/content/data'  # update path as needed\n","\n","print(f\"âœ… Ready with IMG_SIZE={IMG_SIZE}, BATCH_SIZE={BATCH_SIZE}\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"yCQxmIWKFwAD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762852423757,"user_tz":360,"elapsed":41842,"user":{"displayName":"Ayleen J.","userId":"09427723239281310820"}},"outputId":"7fcca4d2-391d-4831-adca-0efa86af538c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Connecting to Google Drive...\n","Mounted at /content/drive\n","Loading metadata...\n","Restructuring data...\n","Searching for all RGB images in Drive...\n","Found 770 actual image files.\n","Successfully matched 770 images with their full paths.\n","\n","Sample of matched data:\n","                   filename    shot_type  label  \\\n","0  -5949315841433628424.png  AT_DISTANCE      0   \n","1  -6837240536182868524.png  AT_DISTANCE      0   \n","2  -1824718034048585128.png  AT_AN_ANGLE      0   \n","3  -2805714129362314533.png  AT_AN_ANGLE      0   \n","4  -4238387650372285498.png     CLOSE_UP      0   \n","\n","                                          image_path  \n","0  /content/drive/Shareddrives/Skinterest-2b/data...  \n","1  /content/drive/Shareddrives/Skinterest-2b/data...  \n","2  /content/drive/Shareddrives/Skinterest-2b/data...  \n","3  /content/drive/Shareddrives/Skinterest-2b/data...  \n","4  /content/drive/Shareddrives/Skinterest-2b/data...  \n","\n","Unique shot types found in CSV:\n","['AT_DISTANCE' 'AT_AN_ANGLE' 'CLOSE_UP']\n","\n","Training samples: 539\n","Validation samples: 115\n","Testing samples: 116\n","                              partial_path    shot_type  \\\n","0  dataset/images/-5949315841433628424.png  AT_DISTANCE   \n","1  dataset/images/-6837240536182868524.png  AT_DISTANCE   \n","2  dataset/images/-1824718034048585128.png  AT_AN_ANGLE   \n","3  dataset/images/-2805714129362314533.png  AT_AN_ANGLE   \n","4  dataset/images/-4238387650372285498.png     CLOSE_UP   \n","\n","                   filename  \\\n","0  -5949315841433628424.png   \n","1  -6837240536182868524.png   \n","2  -1824718034048585128.png   \n","3  -2805714129362314533.png   \n","4  -4238387650372285498.png   \n","\n","                                          image_path  label  \\\n","0  /content/drive/Shareddrives/Skinterest-2b/data...      0   \n","1  /content/drive/Shareddrives/Skinterest-2b/data...      0   \n","2  /content/drive/Shareddrives/Skinterest-2b/data...      0   \n","3  /content/drive/Shareddrives/Skinterest-2b/data...      0   \n","4  /content/drive/Shareddrives/Skinterest-2b/data...      0   \n","\n","   monk_skin_tone_label_us dermatologist_fitzpatrick_skin_type_label_1  \n","0                      1.0                                        FST3  \n","1                      1.0                                        FST2  \n","2                      1.0                                        FST2  \n","3                      1.0                                        FST2  \n","4                      1.0                                        FST2  \n"]}],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","import os\n","import glob\n","from google.colab import drive\n","\n","# --- Mount Drive and Set Paths ---\n","print(\"Connecting to Google Drive...\")\n","drive.mount('/content/drive', force_remount=True)\n","\n","DATA_ROOT_DIR = '/content/drive/Shareddrives/Skinterest-2b/data/Resize_Data/'\n","CSV_PATH = '/content/drive/Shareddrives/Skinterest-2b/data/SCIN_CLEAN.csv'\n","\n","# --- Load Metadata ---\n","print(\"Loading metadata...\")\n","df = pd.read_csv(CSV_PATH)\n","\n","# Clean and convert Monk scale column to integer 1â€“10\n","if 'monk_skin_tone_label_us' in df.columns:\n","    df['monk_skin_tone_label_us'] = (\n","        df['monk_skin_tone_label_us']\n","        .astype(str)                 # ensure consistent string type\n","        .str.extract(r'(\\d+)')       # pull numeric part only\n","        .astype(float)               # convert to numeric\n","        .fillna(0)                   # handle NaN safely\n","        .astype(int)                 # final conversion to int\n","    )\n","\n","\n","# --- Restructure Data (flatten image paths + labels) ---\n","print(\"Restructuring data...\")\n","df1 = df[['image_1_path', 'image_1_shot_type']].rename(\n","    columns={'image_1_path': 'partial_path', 'image_1_shot_type': 'shot_type'}\n",")\n","df2 = df[['image_2_path', 'image_2_shot_type']].rename(\n","    columns={'image_2_path': 'partial_path', 'image_2_shot_type': 'shot_type'}\n",")\n","df3 = df[['image_3_path', 'image_3_shot_type']].rename(\n","    columns={'image_3_path': 'partial_path', 'image_3_shot_type': 'shot_type'}\n",")\n","\n","combined_df = pd.concat([df1, df2, df3], ignore_index=True)\n","combined_df.dropna(subset=['partial_path', 'shot_type'], inplace=True)\n","\n","# Extract filename from path\n","combined_df['filename'] = combined_df['partial_path'].apply(lambda x: os.path.basename(x))\n","\n","# --- Find All Actual RGB Images in Drive ---\n","print(\"Searching for all RGB images in Drive...\")\n","image_paths = glob.glob(\n","    os.path.join(DATA_ROOT_DIR, 'resize_224_split_images_*', 'rgb', '*.png')\n",")\n","print(f\"Found {len(image_paths)} actual image files.\")\n","\n","# Map basename â†’ full path\n","path_map = {os.path.basename(p): p for p in image_paths}\n","\n","# Attach full paths\n","combined_df['image_path'] = combined_df['filename'].map(path_map)\n","\n","# Drop rows without matching files\n","combined_df.dropna(subset=['image_path'], inplace=True)\n","print(f\"Successfully matched {len(combined_df)} images with their full paths.\")\n","\n","# --- Create Binary Label Column ---\n","combined_df['label'] = combined_df['shot_type'].apply(\n","    lambda x: 1 if 'HARSH' in str(x).upper() else 0\n",")\n","\n","\n","# ---- Add metadata columns (lighting, undertone, skin type) ----\n","meta_cols = [\n","    'monk_skin_tone_label_us',\n","    'dermatologist_fitzpatrick_skin_type_label_1'\n","    ]\n","combined_df = combined_df.merge(\n","    df[['image_1_path'] + meta_cols],\n","    left_on='partial_path',\n","    right_on='image_1_path',\n","    how='left'\n",").drop(columns=['image_1_path'])\n","\n","\n","\n","\n","# --- Verify ---\n","print(\"\\nSample of matched data:\")\n","print(combined_df[['filename', 'shot_type', 'label', 'image_path']].head())\n","\n","print(\"\\nUnique shot types found in CSV:\")\n","print(combined_df['shot_type'].unique())\n","\n","# --- Train/Val/Test Split ---\n","final_df = combined_df\n","train_df, test_val_df = train_test_split(\n","    final_df, test_size=0.3, random_state=42, stratify=final_df['label']\n",")\n","val_df, test_df = train_test_split(\n","    test_val_df, test_size=0.5, random_state=42, stratify=test_val_df['label']\n",")\n","\n","print(f\"\\nTraining samples: {len(train_df)}\")\n","print(f\"Validation samples: {len(val_df)}\")\n","print(f\"Testing samples: {len(test_df)}\")\n","print(combined_df.head())\n"]},{"cell_type":"code","source":["# Sanity check, monk is int\n","print(\"Monk column type:\", df['monk_skin_tone_label_us'].dtype)\n","print(\"Unique Monk values:\", sorted(df['monk_skin_tone_label_us'].dropna().unique()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7JBQdHtiLyX2","executionInfo":{"status":"ok","timestamp":1762852423817,"user_tz":360,"elapsed":54,"user":{"displayName":"Ayleen J.","userId":"09427723239281310820"}},"outputId":"5fe95b94-273a-4562-d530-dc9a9d1f5c4b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Monk column type: int64\n","Unique Monk values: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)]\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"RZgWtQFcenOo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762852424555,"user_tz":360,"elapsed":736,"user":{"displayName":"Ayleen J.","userId":"09427723239281310820"}},"outputId":"ca83a0b4-34ff-4839-eade-ea05ba9a8e46"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"5Yb4jFAFMbdE","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1762852424607,"user_tz":360,"elapsed":49,"user":{"displayName":"Ayleen J.","userId":"09427723239281310820"}},"outputId":"f59cdfef-cf4d-4607-cf32-7ca1067ae243"},"outputs":[{"output_type":"stream","name":"stdout","text":["All available column names:\n","Index(['case_id', 'source', 'release', 'year', 'age_group', 'sex_at_birth',\n","       'fitzpatrick_skin_type', 'dermatologist_fitzpatrick_skin_type_label_1',\n","       'dermatologist_fitzpatrick_skin_type_label_2',\n","       'dermatologist_fitzpatrick_skin_type_label_3',\n","       'monk_skin_tone_label_india', 'monk_skin_tone_label_us',\n","       'dermatologist_skin_condition_on_label_name',\n","       'dermatologist_skin_condition_confidence',\n","       'race_ethnicity_american_indian_or_alaska_native',\n","       'race_ethnicity_asian', 'race_ethnicity_black_or_african_american',\n","       'race_ethnicity_hispanic_latino_or_spanish_origin',\n","       'race_ethnicity_middle_eastern_or_north_african',\n","       'race_ethnicity_native_hawaiian_or_pacific_islander',\n","       'race_ethnicity_white', 'race_ethnicity_other_race',\n","       'race_ethnicity_prefer_not_to_answer', 'textures_raised_or_bumpy',\n","       'textures_flat', 'textures_rough_or_flaky', 'textures_fluid_filled',\n","       'body_parts_head_or_neck', 'body_parts_arm', 'body_parts_palm',\n","       'body_parts_back_of_hand', 'body_parts_torso_front',\n","       'body_parts_torso_back', 'body_parts_genitalia_or_groin',\n","       'body_parts_buttocks', 'body_parts_leg', 'body_parts_foot_top_or_side',\n","       'body_parts_foot_sole', 'body_parts_other',\n","       'condition_symptoms_bothersome_appearance',\n","       'condition_symptoms_bleeding', 'condition_symptoms_increasing_size',\n","       'condition_symptoms_darkening', 'condition_symptoms_itching',\n","       'condition_symptoms_burning', 'condition_symptoms_pain',\n","       'condition_symptoms_no_relevant_experience', 'other_symptoms_fever',\n","       'other_symptoms_chills', 'other_symptoms_fatigue',\n","       'other_symptoms_joint_pain', 'other_symptoms_mouth_sores',\n","       'other_symptoms_shortness_of_breath',\n","       'other_symptoms_no_relevant_symptoms', 'related_category',\n","       'condition_duration', 'image_1_path', 'image_2_path', 'image_3_path',\n","       'image_1_shot_type', 'image_2_shot_type', 'image_3_shot_type',\n","       'combined_race', 'race_ethnicity_two_or_more_after_mitigation'],\n","      dtype='object')\n"]}],"source":["import pandas as pd\n","\n","# Make sure this path is correct\n","CSV_PATH = '/content/drive/Shared drives/Skinterest-2b/data/SCIN_CLEAN.csv'\n","df_info = pd.read_csv(CSV_PATH)\n","\n","# This will print all the column names\n","print(\"All available column names:\")\n","print(df_info.columns)"]},{"cell_type":"code","source":["print(combined_df.columns)\n","print(final_df.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ds_83qYE_sdH","executionInfo":{"status":"ok","timestamp":1762852424640,"user_tz":360,"elapsed":32,"user":{"displayName":"Ayleen J.","userId":"09427723239281310820"}},"outputId":"7b6508a7-c572-430c-f48f-d5b8e376dc9e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['partial_path', 'shot_type', 'filename', 'image_path', 'label',\n","       'monk_skin_tone_label_us',\n","       'dermatologist_fitzpatrick_skin_type_label_1'],\n","      dtype='object')\n","Index(['partial_path', 'shot_type', 'filename', 'image_path', 'label',\n","       'monk_skin_tone_label_us',\n","       'dermatologist_fitzpatrick_skin_type_label_1'],\n","      dtype='object')\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"id":"_zF_dOEjVey2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762852424649,"user_tz":360,"elapsed":7,"user":{"displayName":"Ayleen J.","userId":"09427723239281310820"}},"outputId":"02ecd38e-18f9-4916-af67-c238a8acf722"},"outputs":[{"output_type":"stream","name":"stdout","text":["DATA_ROOT_DIR: /content/drive/Shareddrives/Skinterest-2b/data/Resize_Data/\n","\n","Contents of DATA_ROOT_DIR:\n","['.DS_Store', 'resize_224_split_images_6', 'resize_224_split_images_7', 'resize_224_split_images_2', 'resize_224_split_images_3', 'resize_224_split_images_1', 'resize_224_split_images_4', 'resize_224_split_images_5', 'Resize_160', 'resize_224_split_images_8']\n","\n","Contents of resize_224_split_images_1:\n","['.DS_Store', 'rgb', 'hsv', 'lab']\n","\n","Contents of rgb folder (first 10 files):\n","['-1628413175243738425.png', '-1642667040160094612.png', '-1793399078708447490.png', '-2099414475733487459.png', '-1067701776636141108.png', '-1029834132011629215.png', '-2101790873047080311.png', '-1532307918011967845.png', '-1297003684632367094.png', '-144864341463911154.png']\n"]}],"source":["import os\n","\n","DATA_ROOT_DIR = '/content/drive/Shareddrives/Skinterest-2b/data/Resize_Data/'\n","CSV_PATH = '/content/drive/Shareddrives/Skinterest-2b/data/SCIN_CLEAN.csv'\n","\n","# Option A: Use DATA_ROOT_DIR directly\n","print(\"DATA_ROOT_DIR:\", DATA_ROOT_DIR)\n","print(\"\\nContents of DATA_ROOT_DIR:\")\n","print(os.listdir(DATA_ROOT_DIR))\n","\n","split_path = os.path.join(DATA_ROOT_DIR, \"resize_224_split_images_1\")\n","print(\"\\nContents of resize_224_split_images_1:\")\n","print(os.listdir(split_path))\n","\n","rgb_path = os.path.join(split_path, \"rgb\")\n","print(\"\\nContents of rgb folder (first 10 files):\")\n","print(os.listdir(rgb_path)[:10])\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"-s2c_N7OqVWv","executionInfo":{"status":"ok","timestamp":1762852424666,"user_tz":360,"elapsed":15,"user":{"displayName":"Ayleen J.","userId":"09427723239281310820"}}},"outputs":[],"source":["# !pip install tensorflow==2.15"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"qQM5LM1oa0pw","executionInfo":{"status":"ok","timestamp":1762852424670,"user_tz":360,"elapsed":1,"user":{"displayName":"Ayleen J.","userId":"09427723239281310820"}}},"outputs":[],"source":["# Paths\n","csv_path = '/content/drive/Shareddrives/Skinterest-2b/data/SCIN_CLEAN.csv'\n","image_root = '/content/drive/Shareddrives/Skinterest-2b/data/Resize_Data/'"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ncY-UlVP0zTR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762852424680,"user_tz":360,"elapsed":8,"user":{"displayName":"Ayleen J.","userId":"09427723239281310820"}},"outputId":"e4cb050f-e76c-49fd-ab8e-05833fe97771"},"outputs":[{"output_type":"stream","name":"stdout","text":["                              image_1_path                  filename  \\\n","0  dataset/images/-5949315841433628424.png  -5949315841433628424.png   \n","1    dataset/images/325464533153467313.png    325464533153467313.png   \n","2  dataset/images/-6837240536182868524.png  -6837240536182868524.png   \n","3   dataset/images/2983323875335943836.png   2983323875335943836.png   \n","4   dataset/images/3104801012387799539.png   3104801012387799539.png   \n","\n","                                          image_path  \n","0  /content/drive/Shareddrives/Skinterest-2b/data...  \n","1                                                NaN  \n","2  /content/drive/Shareddrives/Skinterest-2b/data...  \n","3                                                NaN  \n","4                                                NaN  \n"]}],"source":["import glob\n","\n","# Collect all rgb images across all split folders\n","all_image_paths = glob.glob(os.path.join(image_root, \"resize_224_split_images_*\", \"rgb\", \"*.png\"))\n","path_map = {os.path.basename(p): p for p in all_image_paths}\n","\n","df['filename'] = df['image_1_path'].apply(lambda x: os.path.basename(str(x)))\n","df['image_path'] = df['filename'].map(path_map)\n","\n","print(df[['image_1_path','filename','image_path']].head())\n","\n"]},{"cell_type":"code","source":["print(combined_df.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hQKP0q_MHoDS","executionInfo":{"status":"ok","timestamp":1762852424687,"user_tz":360,"elapsed":6,"user":{"displayName":"Ayleen J.","userId":"09427723239281310820"}},"outputId":"6d10d3d4-7a4e-452b-a9a4-090fc8919907"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['partial_path', 'shot_type', 'filename', 'image_path', 'label',\n","       'monk_skin_tone_label_us',\n","       'dermatologist_fitzpatrick_skin_type_label_1'],\n","      dtype='object')\n"]}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"OagHSLdtloz5","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1762854002092,"user_tz":360,"elapsed":1268153,"user":{"displayName":"Ayleen J.","userId":"09427723239281310820"}},"outputId":"50e2ef24-4a35-44bf-fe26-84a7f9e2f906"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.19.0\n","Initial combined_df rows: 770\n","Using shot_type as lighting proxy to derive 'lighting_label' (angled vs direct).\n","Using monk_skin_tone_label_us (Monk 1â€“10) for undertone_raw.\n","Using dermatologist_fitzpatrick_skin_type_label_1 (FST1â€“FST6) for skin_type_raw.\n","Undertone classes: ['cool' 'neutral' 'warm']\n","Skin-type classes: ['0' '1' '2' '3' '4' '5' '6' 'nan']\n","Lighting positive count (HARSH=1): 227 / 770\n","lighting_label\n","0    543\n","1    227\n","Name: count, dtype: int64\n","undertone_bucket\n","cool       598\n","neutral    166\n","warm         6\n","Name: count, dtype: int64\n","skin_type_raw\n","nan    235\n","2      202\n","3      144\n","4       65\n","1       60\n","5       34\n","0       27\n","6        3\n","Name: count, dtype: int64\n","Split sizes: 539 115 116\n","Lighting class weights: {0: 0.7092105263157895, 1: 1.6949685534591195}\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"efficientnetb7_multitask\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"efficientnetb7_multitask\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ input_layer_5       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n","â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ \u001b[38;5;34m3\u001b[0m)                â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ efficientnetb7      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2560\u001b[0m)      â”‚ \u001b[38;5;34m64,097,687\u001b[0m â”‚ input_layer_5[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n","â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2560\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ efficientnetb7[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚    \u001b[38;5;34m327,808\u001b[0m â”‚ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚    \u001b[38;5;34m327,808\u001b[0m â”‚ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚    \u001b[38;5;34m327,808\u001b[0m â”‚ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lighting_out        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         â”‚        \u001b[38;5;34m129\u001b[0m â”‚ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n","â”‚ (\u001b[38;5;33mDense\u001b[0m)             â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ undertone_out       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         â”‚        \u001b[38;5;34m387\u001b[0m â”‚ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n","â”‚ (\u001b[38;5;33mDense\u001b[0m)             â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ skin_type_out       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         â”‚      \u001b[38;5;34m1,032\u001b[0m â”‚ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n","â”‚ (\u001b[38;5;33mDense\u001b[0m)             â”‚                   â”‚            â”‚                   â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ input_layer_5       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n","â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ efficientnetb7      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2560</span>)      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64,097,687</span> â”‚ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n","â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2560</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ efficientnetb7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">327,808</span> â”‚ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">327,808</span> â”‚ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">327,808</span> â”‚ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lighting_out        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n","â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ undertone_out       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> â”‚ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n","â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ skin_type_out       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> â”‚ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n","â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚                   â”‚            â”‚                   â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m65,082,659\u001b[0m (248.27 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,082,659</span> (248.27 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m984,972\u001b[0m (3.76 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">984,972</span> (3.76 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m64,097,687\u001b[0m (244.51 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,097,687</span> (244.51 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/8\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 18s/step - lighting_out_acc: 0.6633 - lighting_out_auc: 0.5387 - lighting_out_loss: 0.6600 - loss: 3.3304 - skin_type_out_acc: 0.2197 - skin_type_out_loss: 1.9153 - undertone_out_acc: 0.6656 - undertone_out_loss: 0.7551 - val_lighting_out_acc: 0.6783 - val_lighting_out_auc: 0.4918 - val_lighting_out_loss: 0.6421 - val_loss: 2.9197 - val_skin_type_out_acc: 0.2696 - val_skin_type_out_loss: 1.7819 - val_undertone_out_acc: 0.8174 - val_undertone_out_loss: 0.5254 - learning_rate: 0.0010\n","Epoch 2/8\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - lighting_out_acc: 0.7465 - lighting_out_auc: 0.6648 - lighting_out_loss: 0.5524 - loss: 2.6655 - skin_type_out_acc: 0.3675 - skin_type_out_loss: 1.6042 - undertone_out_acc: 0.7796 - undertone_out_loss: 0.5090 - val_lighting_out_acc: 0.6783 - val_lighting_out_auc: 0.4924 - val_lighting_out_loss: 0.6307 - val_loss: 2.9089 - val_skin_type_out_acc: 0.3043 - val_skin_type_out_loss: 1.7597 - val_undertone_out_acc: 0.7913 - val_undertone_out_loss: 0.5374 - learning_rate: 0.0010\n","Epoch 3/8\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 284ms/step - lighting_out_acc: 0.7084 - lighting_out_auc: 0.6602 - lighting_out_loss: 0.5802 - loss: 2.6035 - skin_type_out_acc: 0.3996 - skin_type_out_loss: 1.5593 - undertone_out_acc: 0.8005 - undertone_out_loss: 0.4637 - val_lighting_out_acc: 0.6870 - val_lighting_out_auc: 0.4700 - val_lighting_out_loss: 0.6380 - val_loss: 2.9175 - val_skin_type_out_acc: 0.2870 - val_skin_type_out_loss: 1.7801 - val_undertone_out_acc: 0.8174 - val_undertone_out_loss: 0.5235 - learning_rate: 0.0010\n","Epoch 4/8\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - lighting_out_acc: 0.7415 - lighting_out_auc: 0.7381 - lighting_out_loss: 0.5272 - loss: 2.4832 - skin_type_out_acc: 0.4596 - skin_type_out_loss: 1.5067 - undertone_out_acc: 0.8083 - undertone_out_loss: 0.4493 - val_lighting_out_acc: 0.6696 - val_lighting_out_auc: 0.4897 - val_lighting_out_loss: 0.6524 - val_loss: 2.9006 - val_skin_type_out_acc: 0.2870 - val_skin_type_out_loss: 1.7609 - val_undertone_out_acc: 0.7826 - val_undertone_out_loss: 0.5082 - learning_rate: 0.0010\n","Epoch 5/8\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - lighting_out_acc: 0.7987 - lighting_out_auc: 0.7717 - lighting_out_loss: 0.4992 - loss: 2.3510 - skin_type_out_acc: 0.4646 - skin_type_out_loss: 1.4271 - undertone_out_acc: 0.8039 - undertone_out_loss: 0.4246 - val_lighting_out_acc: 0.6609 - val_lighting_out_auc: 0.5172 - val_lighting_out_loss: 0.6447 - val_loss: 2.9273 - val_skin_type_out_acc: 0.2783 - val_skin_type_out_loss: 1.7899 - val_undertone_out_acc: 0.8174 - val_undertone_out_loss: 0.5261 - learning_rate: 0.0010\n","Epoch 6/8\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - lighting_out_acc: 0.7821 - lighting_out_auc: 0.8068 - lighting_out_loss: 0.4802 - loss: 2.3696 - skin_type_out_acc: 0.4698 - skin_type_out_loss: 1.4196 - undertone_out_acc: 0.7947 - undertone_out_loss: 0.4696 - val_lighting_out_acc: 0.6087 - val_lighting_out_auc: 0.5100 - val_lighting_out_loss: 0.6625 - val_loss: 2.9493 - val_skin_type_out_acc: 0.2783 - val_skin_type_out_loss: 1.7555 - val_undertone_out_acc: 0.7826 - val_undertone_out_loss: 0.5604 - learning_rate: 0.0010\n","Epoch 7/8\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - lighting_out_acc: 0.7853 - lighting_out_auc: 0.8283 - lighting_out_loss: 0.4712 - loss: 2.1373 - skin_type_out_acc: 0.5321 - skin_type_out_loss: 1.2761 - undertone_out_acc: 0.8271 - undertone_out_loss: 0.3900 - val_lighting_out_acc: 0.6870 - val_lighting_out_auc: 0.5241 - val_lighting_out_loss: 0.6673 - val_loss: 2.9614 - val_skin_type_out_acc: 0.2957 - val_skin_type_out_loss: 1.7588 - val_undertone_out_acc: 0.7739 - val_undertone_out_loss: 0.5600 - learning_rate: 5.0000e-04\n","Epoch 8/8\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 253ms/step - lighting_out_acc: 0.8116 - lighting_out_auc: 0.8364 - lighting_out_loss: 0.4225 - loss: 2.1337 - skin_type_out_acc: 0.4847 - skin_type_out_loss: 1.3168 - undertone_out_acc: 0.8338 - undertone_out_loss: 0.3944 - val_lighting_out_acc: 0.6261 - val_lighting_out_auc: 0.5256 - val_lighting_out_loss: 0.6584 - val_loss: 2.9647 - val_skin_type_out_acc: 0.2783 - val_skin_type_out_loss: 1.7732 - val_undertone_out_acc: 0.7739 - val_undertone_out_loss: 0.5600 - learning_rate: 5.0000e-04\n","Epoch 1/12\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m608s\u001b[0m 21s/step - lighting_out_acc: 0.6526 - lighting_out_auc: 0.6217 - lighting_out_loss: 0.6034 - loss: 3.5979 - skin_type_out_acc: 0.2541 - skin_type_out_loss: 1.9541 - undertone_out_acc: 0.5777 - undertone_out_loss: 1.0401 - val_lighting_out_acc: 0.6957 - val_lighting_out_auc: 0.4659 - val_lighting_out_loss: 0.6353 - val_loss: 2.8837 - val_skin_type_out_acc: 0.2609 - val_skin_type_out_loss: 1.7492 - val_undertone_out_acc: 0.8000 - val_undertone_out_loss: 0.5184 - learning_rate: 5.0000e-05\n","Epoch 2/12\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 675ms/step - lighting_out_acc: 0.6835 - lighting_out_auc: 0.7077 - lighting_out_loss: 0.5939 - loss: 3.1675 - skin_type_out_acc: 0.3086 - skin_type_out_loss: 1.8063 - undertone_out_acc: 0.6737 - undertone_out_loss: 0.7672 - val_lighting_out_acc: 0.7043 - val_lighting_out_auc: 0.4715 - val_lighting_out_loss: 0.6351 - val_loss: 2.8726 - val_skin_type_out_acc: 0.2609 - val_skin_type_out_loss: 1.7421 - val_undertone_out_acc: 0.8087 - val_undertone_out_loss: 0.5109 - learning_rate: 5.0000e-05\n","Epoch 3/12\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 306ms/step - lighting_out_acc: 0.7376 - lighting_out_auc: 0.7629 - lighting_out_loss: 0.5428 - loss: 2.8627 - skin_type_out_acc: 0.4020 - skin_type_out_loss: 1.6787 - undertone_out_acc: 0.7770 - undertone_out_loss: 0.6411 - val_lighting_out_acc: 0.6957 - val_lighting_out_auc: 0.4697 - val_lighting_out_loss: 0.6368 - val_loss: 2.8827 - val_skin_type_out_acc: 0.2522 - val_skin_type_out_loss: 1.7489 - val_undertone_out_acc: 0.8261 - val_undertone_out_loss: 0.5093 - learning_rate: 5.0000e-05\n","Epoch 4/12\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - lighting_out_acc: 0.7509 - lighting_out_auc: 0.7960 - lighting_out_loss: 0.5030 - loss: 2.5981 - skin_type_out_acc: 0.4274 - skin_type_out_loss: 1.5820 - undertone_out_acc: 0.8376 - undertone_out_loss: 0.5131 - val_lighting_out_acc: 0.6870 - val_lighting_out_auc: 0.4577 - val_lighting_out_loss: 0.6429 - val_loss: 2.9060 - val_skin_type_out_acc: 0.2522 - val_skin_type_out_loss: 1.7550 - val_undertone_out_acc: 0.8348 - val_undertone_out_loss: 0.5147 - learning_rate: 5.0000e-05\n","Epoch 5/12\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - lighting_out_acc: 0.7883 - lighting_out_auc: 0.7887 - lighting_out_loss: 0.5045 - loss: 2.4035 - skin_type_out_acc: 0.4698 - skin_type_out_loss: 1.4894 - undertone_out_acc: 0.8741 - undertone_out_loss: 0.4092 - val_lighting_out_acc: 0.6870 - val_lighting_out_auc: 0.4637 - val_lighting_out_loss: 0.6485 - val_loss: 2.9349 - val_skin_type_out_acc: 0.2522 - val_skin_type_out_loss: 1.7596 - val_undertone_out_acc: 0.8174 - val_undertone_out_loss: 0.5299 - learning_rate: 5.0000e-05\n","Epoch 6/12\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - lighting_out_acc: 0.7936 - lighting_out_auc: 0.8279 - lighting_out_loss: 0.4851 - loss: 2.3679 - skin_type_out_acc: 0.5055 - skin_type_out_loss: 1.4512 - undertone_out_acc: 0.8343 - undertone_out_loss: 0.4316 - val_lighting_out_acc: 0.6783 - val_lighting_out_auc: 0.4775 - val_lighting_out_loss: 0.6496 - val_loss: 2.9505 - val_skin_type_out_acc: 0.2870 - val_skin_type_out_loss: 1.7621 - val_undertone_out_acc: 0.8174 - val_undertone_out_loss: 0.5368 - learning_rate: 2.5000e-05\n","Epoch 7/12\n","\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - lighting_out_acc: 0.8131 - lighting_out_auc: 0.8941 - lighting_out_loss: 0.4414 - loss: 2.2529 - skin_type_out_acc: 0.5031 - skin_type_out_loss: 1.4407 - undertone_out_acc: 0.8816 - undertone_out_loss: 0.3709 - val_lighting_out_acc: 0.6609 - val_lighting_out_auc: 0.4922 - val_lighting_out_loss: 0.6475 - val_loss: 2.9640 - val_skin_type_out_acc: 0.2783 - val_skin_type_out_loss: 1.7670 - val_undertone_out_acc: 0.8174 - val_undertone_out_loss: 0.5454 - learning_rate: 2.5000e-05\n","Evaluating on test set...\n","\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 31s/step\n","Lighting metrics (binary):\n","Accuracy: 0.6637931034482759\n","Precision: 0.0\n","Recall (sensitivity): 0.0\n","Specificity: 0.9390243902439024\n","F1: 0.0\n","\n","Undertone classification report:\n","              precision    recall  f1-score   support\n","\n","        cool       0.79      0.94      0.86        88\n","     neutral       0.45      0.19      0.27        26\n","        warm       0.00      0.00      0.00         2\n","\n","    accuracy                           0.76       116\n","   macro avg       0.42      0.38      0.38       116\n","weighted avg       0.70      0.76      0.71       116\n","\n","\n","Skin-type classification report:\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         4\n","           1       0.00      0.00      0.00         6\n","           2       0.30      0.36      0.33        28\n","           3       0.20      0.08      0.12        24\n","           4       0.00      0.00      0.00        10\n","           5       0.00      0.00      0.00         6\n","           6       0.00      0.00      0.00         1\n","         nan       0.33      0.65      0.44        37\n","\n","    accuracy                           0.31       116\n","   macro avg       0.10      0.14      0.11       116\n","weighted avg       0.22      0.31      0.24       116\n","\n","\n","Sensitivity/Specificity by dermatologist_fitzpatrick_skin_type_label_1:\n","group | n | sensitivity | specificity | tp | fn | tn | fp\n","('nan', 37, np.float64(0.0), np.float64(0.9629629629629629), np.int64(0), np.int64(10), np.int64(26), np.int64(1))\n","('FST2', 28, np.float64(0.0), np.float64(0.8235294117647058), np.int64(0), np.int64(11), np.int64(14), np.int64(3))\n","('FST3', 24, np.float64(0.0), np.float64(0.9411764705882353), np.int64(0), np.int64(7), np.int64(16), np.int64(1))\n","('FST4', 10, np.float64(0.0), np.float64(1.0), np.int64(0), np.int64(1), np.int64(9), np.int64(0))\n","('FST1', 6, np.float64(0.0), np.float64(1.0), np.int64(0), np.int64(3), np.int64(3), np.int64(0))\n","('FST5', 6, np.float64(0.0), np.float64(1.0), np.int64(0), np.int64(2), np.int64(4), np.int64(0))\n","('0', 4, None, np.float64(1.0), np.int64(0), np.int64(0), np.int64(4), np.int64(0))\n","('FST6', 1, None, np.float64(1.0), np.int64(0), np.int64(0), np.int64(1), np.int64(0))\n","Saved model weights to: /content/drive/MyDrive/skinterest_multitask_final.weights.h5\n"]}],"source":["# Full multi-head ResNet152V2 fine-tuning pipeline\n","# Run this after you've already produced combined_df that contains:\n","#   'filename', 'image_path', and CSV columns from SCIN_CLEAN.\n","# Adjust column names in the CONFIG section if your CSV uses different names.\n","\n","import os\n","import math\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras import layers, models, losses, optimizers, callbacks\n","from tensorflow.keras.applications import EfficientNetB7\n","from tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","# ---------- CONFIG ----------\n","IMAGE_SIZE = (224, 224)\n","BATCH_SIZE = 32\n","AUTO = tf.data.AUTOTUNE\n","EPOCHS_HEADS = 8        # train heads first\n","EPOCHS_UNFREEZE = 12    # then unfreeze top and train more\n","LEARNING_RATE_HEADS = 1e-3\n","LEARNING_RATE_FINETUNE = 5e-5\n","MODEL_INPUT_SHAPE = IMAGE_SIZE + (3,)\n","\n","# Column names in your dataframe (change if different)\n","COL_IMAGE_PATH = 'image_path'   # full path we already produced\n","# For lighting quality (binary): set to column in CSV that indicates harsh lighting.\n","# If none exists, the code will try 'image_?_shot_type' but will fallback to placeholder.\n","COL_LIGHTING_SOURCE = 'shot_type'  # example; change to real lighting column if available\n","\n","# Undertone column (multi-class). Example candidates: 'monk_skin_tone_label_us' or 'monk_skin_tone_label_india'\n","COL_UNDERTONE = 'monk_skin_tone_label_us'   # change as required\n","\n","# Skin type (Fitzpatrick) column (multi-class)\n","COL_SKIN_TYPE = 'dermatologist_fitzpatrick_skin_type_label_1'  # change as required\n","\n","# Grouping column for evaluation (sensitivity/specificity per group),\n","# typically fitzpatrick or monk skin tone. Set to one of the above or None.\n","EVAL_GROUP_COL = COL_SKIN_TYPE\n","\n","# Loss weights for multi-task loss (you can tune these)\n","LOSS_WEIGHTS = {\n","    'lighting_out': 1.0,\n","    'undertone_out': 1.0,\n","    'skin_type_out': 1.0\n","}\n","\n","# ---------- Helpers & Sanity checks ----------\n","print(\"TensorFlow version:\", tf.__version__)\n","tf.keras.mixed_precision.set_global_policy('mixed_float16')  # speedup on GPU\n","\n","# You must have `combined_df` already in the workspace. If not, load it first:\n","try:\n","    combined_df\n","except NameError:\n","    raise RuntimeError(\"combined_df not found in workspace. Run your preprocessing cell that builds combined_df first.\")\n","\n","print(\"Initial combined_df rows:\", len(combined_df))\n","\n","# Ensure image paths exist â€” filter out missing files\n","exists_mask = combined_df[COL_IMAGE_PATH].apply(lambda p: os.path.exists(p))\n","if not exists_mask.all():\n","    missing = (~exists_mask).sum()\n","    print(f\"Warning: {missing} entries have missing image file paths; they will be dropped.\")\n","    combined_df = combined_df[exists_mask].copy()\n","\n","# ---------- Create target columns ----------\n","# LIGHTING (binary): derived from shot type proxy\n","if COL_LIGHTING_SOURCE in combined_df.columns:\n","    print(f\"Using {COL_LIGHTING_SOURCE} as lighting proxy to derive 'lighting_label' (angled vs direct).\")\n","    def map_shot_to_light(x):\n","        if isinstance(x, str):\n","            x = x.upper()\n","            if 'ANGLE' in x:\n","                return 1  # proxy for harsh\n","            elif 'DISTANCE' in x:\n","                return 0  # neutral\n","            elif 'CLOSE' in x:\n","                return 0  # neutral\n","        return 0\n","    combined_df['lighting_label'] = combined_df[COL_LIGHTING_SOURCE].apply(map_shot_to_light)\n","else:\n","    print(f\"Note: {COL_LIGHTING_SOURCE} not found. Creating placeholder 'lighting_label' = 0 for all.\")\n","    combined_df['lighting_label'] = 0\n","\n","\n","# UNDERTONE (Monk scale: 1â€“10)\n","# Handle potential NaNs before converting to int\n","if COL_UNDERTONE in combined_df.columns:\n","    combined_df[COL_UNDERTONE] = combined_df[COL_UNDERTONE].fillna(0).astype(int) # Fill NaN and convert to int\n","    print(f\"Using {COL_UNDERTONE} (Monk 1â€“10) for undertone_raw.\")\n","    combined_df['undertone_raw'] = combined_df[COL_UNDERTONE].astype(str).str.strip()\n","    # Optional: bucket Monk 1â€“10 into cool/warm/neutral\n","    def monk_to_undertone(x):\n","        try:\n","            x = int(x)\n","            if x <= 3:\n","                return 'cool'\n","            elif 4 <= x <= 7:\n","                return 'neutral'\n","            else:\n","                return 'warm'\n","        except:\n","            return 'unknown'\n","    combined_df['undertone_bucket'] = combined_df['undertone_raw'].apply(monk_to_undertone)\n","else:\n","    print(\"No undertone column found. Creating placeholder 'undertone_raw' with single class 'unknown'.\")\n","    combined_df['undertone_raw'] = 'unknown'\n","    combined_df['undertone_bucket'] = 'unknown'\n","\n","\n","# SKIN TYPE (Fitzpatrick: FST1â€“FST6)\n","if COL_SKIN_TYPE in combined_df.columns and combined_df[COL_SKIN_TYPE].notna().any():\n","    print(f\"Using {COL_SKIN_TYPE} (FST1â€“FST6) for skin_type_raw.\")\n","    combined_df['skin_type_raw'] = combined_df[COL_SKIN_TYPE].astype(str).str.replace('FST', '', regex=False).str.strip()\n","else:\n","    print(\"No Fitzpatrick column found. Creating placeholder 'skin_type_raw' with single class 'unknown'.\")\n","    combined_df['skin_type_raw'] = 'unknown'\n","\n","\n","# ---------- Encode categorical targets ----------\n","le_undertone = LabelEncoder()\n","combined_df['undertone_id'] = le_undertone.fit_transform(combined_df['undertone_bucket'].fillna('unknown'))\n","\n","le_skin = LabelEncoder()\n","combined_df['skin_type_id'] = le_skin.fit_transform(combined_df['skin_type_raw'].fillna('unknown'))\n","\n","print(\"Undertone classes:\", le_undertone.classes_)\n","print(\"Skin-type classes:\", le_skin.classes_)\n","print(\"Lighting positive count (HARSH=1):\", combined_df['lighting_label'].sum(), \"/\", len(combined_df))\n","\n","print(combined_df['lighting_label'].value_counts())\n","print(combined_df['undertone_bucket'].value_counts())\n","print(combined_df['skin_type_raw'].value_counts())\n","\n","\n","\n","# ---------- Train/Val/Test split ----------\n","from sklearn.model_selection import train_test_split\n","final_df = combined_df.copy()\n","train_df, test_val_df = train_test_split(final_df, test_size=0.3, random_state=42, stratify=final_df['lighting_label'])\n","val_df, test_df = train_test_split(test_val_df, test_size=0.5, random_state=42, stratify=test_val_df['lighting_label'])\n","\n","print(\"Split sizes:\", len(train_df), len(val_df), len(test_df))\n","\n","# ---------- Build tf.data pipelines ----------\n","def decode_and_preprocess(path, label_lighting, label_undertone, label_skin):\n","    # read\n","    image = tf.io.read_file(path)\n","    image = tf.image.decode_image(image, channels=3)  # handles png/jpg\n","    image.set_shape([None, None, 3])\n","    # convert to float32\n","    image = tf.image.convert_image_dtype(image, tf.float32)\n","    # resize\n","    image = tf.image.resize(image, IMAGE_SIZE)\n","    # preprocessing for EfficientNet: normalization to match pretrained weights: scale to [-1,1]\n","    image = effnet_preprocess(image * 255.0)  # resnet preprocess expects [0,255] input\n","    return image, {'lighting_out': label_lighting, 'undertone_out': label_undertone, 'skin_type_out': label_skin}\n","\n","# Data augmentation (applied only to training)\n","data_augmentation = tf.keras.Sequential([\n","    layers.RandomFlip(\"horizontal\"),\n","    layers.RandomRotation(0.08),\n","    layers.RandomZoom(0.08),\n","    # Add more if needed\n","], name=\"data_augmentation\")\n","\n","def preprocess_for_train(path, lighting, undertone, skin):\n","    image, labels = decode_and_preprocess(path, lighting, undertone, skin)\n","    image = data_augmentation(image)\n","    return image, labels\n","\n","def preprocess_for_eval(path, lighting, undertone, skin):\n","    image, labels = decode_and_preprocess(path, lighting, undertone, skin)\n","    return image, labels\n","\n","def make_dataset(df, shuffle=False, batch_size=BATCH_SIZE, training=False):\n","    paths = df[COL_IMAGE_PATH].values\n","    lighting = df['lighting_label'].astype(np.int32).values\n","    undertone = df['undertone_id'].astype(np.int32).values\n","    skin = df['skin_type_id'].astype(np.int32).values\n","\n","    ds = tf.data.Dataset.from_tensor_slices((paths, lighting, undertone, skin))\n","    if shuffle:\n","        ds = ds.shuffle(buffer_size=len(paths))\n","    map_fn = preprocess_for_train if training else preprocess_for_eval\n","    ds = ds.map(map_fn, num_parallel_calls=AUTO)\n","    ds = ds.batch(batch_size)\n","    ds = ds.prefetch(AUTO)\n","    return ds\n","\n","train_ds = make_dataset(train_df, shuffle=True, training=True)\n","val_ds = make_dataset(val_df, shuffle=False, training=False)\n","test_ds = make_dataset(test_df, shuffle=False, training=False)\n","\n","# ---------- Compute class weights for lighting (binary) to help imbalance ----------\n","classes = np.unique(train_df['lighting_label'])\n","class_weights = {}\n","if len(classes) > 1:\n","    weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df['lighting_label'].values)\n","    class_weights = {int(c): float(w) for c, w in zip(classes, weights)}\n","    print(\"Lighting class weights:\", class_weights)\n","else:\n","    print(\"Only one class present for lighting in train set; class weighting skipped.\")\n","    class_weights = None\n","\n","# ---------- Build the multi-head model ----------\n","def build_model(num_undertone_classes, num_skin_classes, input_shape=MODEL_INPUT_SHAPE):\n","    # Backbone without top\n","    base = EfficientNetB7(include_top=False, input_shape=input_shape, weights='imagenet', pooling='avg')\n","    base.trainable = False  # freeze initially (Phase A)\n","\n","    inputs = layers.Input(shape=input_shape)\n","    x = inputs\n","    # base preprocessing already applied in pipeline\n","    x = base(x, training=False)\n","    x = layers.Dropout(0.4)(x)\n","\n","    # Lighting head (binary)\n","    lighting = layers.Dense(128, activation='relu')(x)\n","    lighting = layers.Dropout(0.3)(lighting)\n","    lighting_out = layers.Dense(1, activation='sigmoid', dtype='float32', name='lighting_out')(lighting)\n","\n","    # Undertone head (multiclass)\n","    ud = layers.Dense(128, activation='relu')(x)\n","    ud = layers.Dropout(0.3)(ud)\n","    undertone_out = layers.Dense(num_undertone_classes, activation='softmax', dtype='float32', name='undertone_out')(ud)\n","\n","    # Skin-type head (multiclass)\n","    st = layers.Dense(128, activation='relu')(x)\n","    st = layers.Dropout(0.3)(st)\n","    skin_type_out = layers.Dense(num_skin_classes, activation='softmax', dtype='float32', name='skin_type_out')(st)\n","\n","    model = models.Model(inputs=inputs, outputs=[lighting_out, undertone_out, skin_type_out], name=\"efficientnetb7_multitask\")\n","    return model\n","\n","num_undertone_classes = len(le_undertone.classes_)\n","num_skin_classes = len(le_skin.classes_)\n","model = build_model(num_undertone_classes, num_skin_classes)\n","model.summary()\n","\n","# ---------- Compile for head training ----------\n","losses_dict = {\n","    'lighting_out': losses.BinaryCrossentropy(),\n","    'undertone_out': losses.SparseCategoricalCrossentropy(),\n","    'skin_type_out': losses.SparseCategoricalCrossentropy()\n","}\n","\n","metrics_dict = {\n","    'lighting_out': [tf.keras.metrics.BinaryAccuracy(name='acc'), tf.keras.metrics.AUC(name='auc')],\n","    'undertone_out': [tf.keras.metrics.SparseCategoricalAccuracy(name='acc')],\n","    'skin_type_out': [tf.keras.metrics.SparseCategoricalAccuracy(name='acc')]\n","}\n","\n","opt = optimizers.Adam(learning_rate=LEARNING_RATE_HEADS)\n","model.compile(optimizer=opt, loss=losses_dict, loss_weights=LOSS_WEIGHTS, metrics=metrics_dict)\n","\n","# ---------- Callbacks ----------\n","ckpt_path = \"/content/drive/MyDrive/skinterest_efficientnetb7_multitask_ckpt.weights.h5\"\n","\n","cb = [\n","    callbacks.ModelCheckpoint(\n","        ckpt_path,\n","        monitor='val_lighting_out_acc',\n","        save_best_only=True,\n","        save_weights_only=True,\n","        mode='max'\n","    ),\n","    callbacks.EarlyStopping(\n","        monitor='val_lighting_out_acc',\n","        patience=5,\n","        restore_best_weights=True,\n","        mode='max'\n","    ),\n","    callbacks.ReduceLROnPlateau(\n","        monitor='val_lighting_out_acc',\n","        factor=0.5,\n","        patience=3,\n","        mode='max'\n","    )\n","]\n","\n","\n","\n","# ---------- Train heads ----------\n","history_heads = model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=EPOCHS_HEADS,\n","    callbacks=cb,\n","    # Keras does class_weight only supports single-output mapping. We'll handle lighting class imbalance using sample weighting trick if needed.\n",")\n","\n","# ---------- Unfreeze top of backbone for fine-tuning ----------\n","# Unfreeze last N layers of base\n","# ---------- Unfreeze top of backbone for fine-tuning ----------\n","base = model.get_layer(\"efficientnetb7\")  # <- directly fetch backbone by name\n","\n","if base is None:\n","    print(\"Warning: could not find backbone to unfreeze; skipping fine-tuning.\")\n","else:\n","    base.trainable = True\n","    # Freeze earlier layers, unfreeze last N\n","    fine_tune_at = int(len(base.layers) * 0.6)  # unfreeze top 40%\n","    for i, layer in enumerate(base.layers):\n","        layer.trainable = i >= fine_tune_at\n","\n","    # Recompile with lower LR\n","    opt_finetune = optimizers.Adam(learning_rate=LEARNING_RATE_FINETUNE)\n","    model.compile(optimizer=opt_finetune, loss=losses_dict, loss_weights=LOSS_WEIGHTS, metrics=metrics_dict)\n","\n","    history_ft = model.fit(\n","        train_ds,\n","        validation_data=val_ds,\n","        epochs=EPOCHS_UNFREEZE,\n","        callbacks=cb\n","    )\n","\n","# ---------- Evaluation on test set ----------\n","print(\"Evaluating on test set...\")\n","test_pred = model.predict(test_ds, verbose=1)\n","\n","# test_pred is list: [lighting_probs, undertone_probs, skin_probs]\n","lighting_probs = np.asarray(test_pred[0]).squeeze()\n","undertone_probs = np.asarray(test_pred[1])\n","skin_probs = np.asarray(test_pred[2])\n","\n","lighting_preds = (lighting_probs >= 0.5).astype(int)\n","# convert undertone and skin preds to labels\n","undertone_preds = np.argmax(undertone_probs, axis=1)\n","skin_preds = np.argmax(skin_probs, axis=1)\n","\n","# Ground truth arrays\n","y_lighting = test_df['lighting_label'].astype(int).values\n","y_undertone = test_df['undertone_id'].astype(int).values\n","y_skin = test_df['skin_type_id'].astype(int).values\n","\n","# Overall metrics\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","print(\"Lighting metrics (binary):\")\n","print(\"Accuracy:\", accuracy_score(y_lighting, lighting_preds))\n","print(\"Precision:\", precision_score(y_lighting, lighting_preds, zero_division=0))\n","print(\"Recall (sensitivity):\", recall_score(y_lighting, lighting_preds, zero_division=0))\n","print(\"Specificity:\", None if len(np.unique(y_lighting))<2 else \\\n","      (confusion_matrix(y_lighting, lighting_preds)[0,0] / (confusion_matrix(y_lighting, lighting_preds)[0,0] + confusion_matrix(y_lighting, lighting_preds)[0,1])))\n","print(\"F1:\", f1_score(y_lighting, lighting_preds, zero_division=0))\n","\n","print(\"\\nUndertone classification report:\")\n","print(classification_report(y_undertone, undertone_preds, target_names=le_undertone.classes_, zero_division=0))\n","\n","print(\"\\nSkin-type classification report:\")\n","print(classification_report(y_skin, skin_preds, target_names=le_skin.classes_, zero_division=0))\n","\n","# ---------- Sensitivity/Specificity by group ----------\n","def sens_spec_by_group(y_true_binary, y_pred_binary, group_values, group_name):\n","    df_eval = pd.DataFrame({'y_true': y_true_binary, 'y_pred': y_pred_binary, 'group': group_values})\n","    groups = df_eval['group'].unique()\n","    rows = []\n","    for g in groups:\n","        sub = df_eval[df_eval['group'] == g]\n","        if len(sub) == 0:\n","            continue\n","        cm = confusion_matrix(sub['y_true'], sub['y_pred'], labels=[0,1])\n","        tn, fp, fn, tp = (cm.ravel() if cm.size==4 else (cm[0,0], 0, 0, cm[0,0]))\n","        sens = tp / (tp + fn) if (tp + fn) > 0 else None\n","        spec = tn / (tn + fp) if (tn + fp) > 0 else None\n","        rows.append((g, len(sub), sens, spec, tp, fn, tn, fp))\n","    rows = sorted(rows, key=lambda r: r[1], reverse=True)\n","    print(f\"\\nSensitivity/Specificity by {group_name}:\")\n","    print(\"group | n | sensitivity | specificity | tp | fn | tn | fp\")\n","    for r in rows:\n","        print(r)\n","\n","# If EVAL_GROUP_COL exists in test_df, compute sensitivity/specificity for lighting across groups\n","if EVAL_GROUP_COL in test_df.columns:\n","    group_vals = test_df[EVAL_GROUP_COL].astype(str).values\n","    sens_spec_by_group(y_lighting, lighting_preds, group_vals, EVAL_GROUP_COL)\n","else:\n","    print(f\"Eval group column {EVAL_GROUP_COL} not present in test_df; skipping per-group sensitivity/specificity.\")\n","\n","# Save final model weights (optional)\n","final_weights_path = \"/content/drive/MyDrive/skinterest_multitask_final.weights.h5\"\n","model.save_weights(final_weights_path)\n","print(\"Saved model weights to:\", final_weights_path)\n","\n","# ---------- End ----------"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}