{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V5E1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install tensorflow --upgrade\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xzy0VIRnpedV","executionInfo":{"status":"ok","timestamp":1759538225051,"user_tz":300,"elapsed":43664,"user":{"displayName":"Aisha Salimgereyeva","userId":"12605674983793144003"}},"outputId":"52f5d81f-a76d-4655-a8ef-293c9767f6e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow\n","  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n","Collecting astunparse>=1.6.0 (from tensorflow)\n","  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n","Collecting flatbuffers>=24.3.25 (from tensorflow)\n","  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n","Collecting google_pasta>=0.1.1 (from tensorflow)\n","  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n","Collecting libclang>=13.0.0 (from tensorflow)\n","  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n","Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (6.32.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n","Collecting tensorboard~=2.20.0 (from tensorflow)\n","  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n","Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n","  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.3.6)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n","Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n","  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n","Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n","  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n","Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m759.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n","Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n","Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m147.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m162.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorboard-data-server, google_pasta, tensorboard, astunparse, tensorflow\n","Successfully installed astunparse-1.6.3 flatbuffers-25.9.23 google_pasta-0.2.0 libclang-18.1.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 werkzeug-3.1.3 wheel-0.45.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCQxmIWKFwAD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759541191579,"user_tz":300,"elapsed":1671,"user":{"displayName":"Aisha Salimgereyeva","userId":"12605674983793144003"}},"outputId":"e8be4dc0-f2af-420a-fc89-22932d0081ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Connecting to Google Drive...\n","Mounted at /content/drive\n","Loading metadata...\n","Restructuring data...\n","Searching for all RGB images in Drive...\n","Found 770 actual image files.\n","Successfully matched 770 images with their full paths.\n","\n","Sample of matched data:\n","                   filename   label_text  label  \\\n","0  -5949315841433628424.png  AT_DISTANCE      0   \n","2  -6837240536182868524.png  AT_DISTANCE      0   \n","5  -1824718034048585128.png  AT_AN_ANGLE      0   \n","6  -2805714129362314533.png  AT_AN_ANGLE      0   \n","9  -4238387650372285498.png     CLOSE_UP      0   \n","\n","                                          image_path  \n","0  /content/drive/Shareddrives/Skinterest-2b/data...  \n","2  /content/drive/Shareddrives/Skinterest-2b/data...  \n","5  /content/drive/Shareddrives/Skinterest-2b/data...  \n","6  /content/drive/Shareddrives/Skinterest-2b/data...  \n","9  /content/drive/Shareddrives/Skinterest-2b/data...  \n","\n","Unique shot types found in CSV:\n","['AT_DISTANCE' 'AT_AN_ANGLE' 'CLOSE_UP']\n","\n","Training samples: 539\n","Validation samples: 115\n","Testing samples: 116\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","import os\n","import glob\n","from google.colab import drive\n","\n","# --- Mount Drive and Set Paths ---\n","print(\"Connecting to Google Drive...\")\n","drive.mount('/content/drive', force_remount=True)\n","\n","DATA_ROOT_DIR = '/content/drive/Shareddrives/Skinterest-2b/data/Resize_Data/'\n","CSV_PATH = '/content/drive/Shareddrives/Skinterest-2b/data/SCIN_CLEAN.csv'\n","\n","# --- Load Metadata ---\n","print(\"Loading metadata...\")\n","df = pd.read_csv(CSV_PATH)\n","\n","# --- Restructure Data (flatten image paths + labels) ---\n","print(\"Restructuring data...\")\n","df1 = df[['image_1_path', 'image_1_shot_type']].rename(\n","    columns={'image_1_path': 'partial_path', 'image_1_shot_type': 'label_text'}\n",")\n","df2 = df[['image_2_path', 'image_2_shot_type']].rename(\n","    columns={'image_2_path': 'partial_path', 'image_2_shot_type': 'label_text'}\n",")\n","df3 = df[['image_3_path', 'image_3_shot_type']].rename(\n","    columns={'image_3_path': 'partial_path', 'image_3_shot_type': 'label_text'}\n",")\n","\n","combined_df = pd.concat([df1, df2, df3], ignore_index=True)\n","combined_df.dropna(subset=['partial_path', 'label_text'], inplace=True)\n","\n","# Extract filename from path\n","combined_df['filename'] = combined_df['partial_path'].apply(lambda x: os.path.basename(x))\n","\n","# --- Find All Actual RGB Images in Drive ---\n","print(\"Searching for all RGB images in Drive...\")\n","image_paths = glob.glob(\n","    os.path.join(DATA_ROOT_DIR, 'resize_224_split_images_*', 'rgb', '*.png')\n",")\n","print(f\"Found {len(image_paths)} actual image files.\")\n","\n","# Map basename → full path\n","path_map = {os.path.basename(p): p for p in image_paths}\n","\n","# Attach full paths\n","combined_df['image_path'] = combined_df['filename'].map(path_map)\n","\n","# Drop rows without matching files\n","combined_df.dropna(subset=['image_path'], inplace=True)\n","print(f\"Successfully matched {len(combined_df)} images with their full paths.\")\n","\n","# --- Create Binary Label Column ---\n","combined_df['label'] = combined_df['label_text'].apply(\n","    lambda x: 1 if 'HARSH' in str(x).upper() else 0\n",")\n","\n","# --- Verify ---\n","print(\"\\nSample of matched data:\")\n","print(combined_df[['filename', 'label_text', 'label', 'image_path']].head())\n","\n","print(\"\\nUnique shot types found in CSV:\")\n","print(combined_df['label_text'].unique())\n","\n","# --- Train/Val/Test Split ---\n","final_df = combined_df\n","train_df, test_val_df = train_test_split(\n","    final_df, test_size=0.3, random_state=42, stratify=final_df['label']\n",")\n","val_df, test_df = train_test_split(\n","    test_val_df, test_size=0.5, random_state=42, stratify=test_val_df['label']\n",")\n","\n","print(f\"\\nTraining samples: {len(train_df)}\")\n","print(f\"Validation samples: {len(val_df)}\")\n","print(f\"Testing samples: {len(test_df)}\")\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZgWtQFcenOo","executionInfo":{"status":"ok","timestamp":1759538327686,"user_tz":300,"elapsed":7497,"user":{"displayName":"Aisha Salimgereyeva","userId":"12605674983793144003"}},"outputId":"abd8c377-78f1-4471-d2d7-31376d95820a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Make sure this path is correct\n","CSV_PATH = '/content/drive/Shared drives/Skinterest-2b/data/SCIN_CLEAN.csv'\n","df_info = pd.read_csv(CSV_PATH)\n","\n","# This will print all the column names\n","print(\"All available column names:\")\n","print(df_info.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Yb4jFAFMbdE","executionInfo":{"status":"ok","timestamp":1759541197275,"user_tz":300,"elapsed":16,"user":{"displayName":"Aisha Salimgereyeva","userId":"12605674983793144003"}},"outputId":"d6ae50a5-fbac-4358-be1e-ff0d4b6cd991"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All available column names:\n","Index(['case_id', 'source', 'release', 'year', 'age_group', 'sex_at_birth',\n","       'fitzpatrick_skin_type', 'dermatologist_fitzpatrick_skin_type_label_1',\n","       'dermatologist_fitzpatrick_skin_type_label_2',\n","       'dermatologist_fitzpatrick_skin_type_label_3',\n","       'monk_skin_tone_label_india', 'monk_skin_tone_label_us',\n","       'dermatologist_skin_condition_on_label_name',\n","       'dermatologist_skin_condition_confidence',\n","       'race_ethnicity_american_indian_or_alaska_native',\n","       'race_ethnicity_asian', 'race_ethnicity_black_or_african_american',\n","       'race_ethnicity_hispanic_latino_or_spanish_origin',\n","       'race_ethnicity_middle_eastern_or_north_african',\n","       'race_ethnicity_native_hawaiian_or_pacific_islander',\n","       'race_ethnicity_white', 'race_ethnicity_other_race',\n","       'race_ethnicity_prefer_not_to_answer', 'textures_raised_or_bumpy',\n","       'textures_flat', 'textures_rough_or_flaky', 'textures_fluid_filled',\n","       'body_parts_head_or_neck', 'body_parts_arm', 'body_parts_palm',\n","       'body_parts_back_of_hand', 'body_parts_torso_front',\n","       'body_parts_torso_back', 'body_parts_genitalia_or_groin',\n","       'body_parts_buttocks', 'body_parts_leg', 'body_parts_foot_top_or_side',\n","       'body_parts_foot_sole', 'body_parts_other',\n","       'condition_symptoms_bothersome_appearance',\n","       'condition_symptoms_bleeding', 'condition_symptoms_increasing_size',\n","       'condition_symptoms_darkening', 'condition_symptoms_itching',\n","       'condition_symptoms_burning', 'condition_symptoms_pain',\n","       'condition_symptoms_no_relevant_experience', 'other_symptoms_fever',\n","       'other_symptoms_chills', 'other_symptoms_fatigue',\n","       'other_symptoms_joint_pain', 'other_symptoms_mouth_sores',\n","       'other_symptoms_shortness_of_breath',\n","       'other_symptoms_no_relevant_symptoms', 'related_category',\n","       'condition_duration', 'image_1_path', 'image_2_path', 'image_3_path',\n","       'image_1_shot_type', 'image_2_shot_type', 'image_3_shot_type',\n","       'combined_race', 'race_ethnicity_two_or_more_after_mitigation'],\n","      dtype='object')\n"]}]},{"cell_type":"code","source":["import os\n","\n","DATA_ROOT_DIR = '/content/drive/Shareddrives/Skinterest-2b/data/Resize_Data/'\n","CSV_PATH = '/content/drive/Shareddrives/Skinterest-2b/data/SCIN_CLEAN.csv'\n","\n","# Option A: Use DATA_ROOT_DIR directly\n","print(\"DATA_ROOT_DIR:\", DATA_ROOT_DIR)\n","print(\"\\nContents of DATA_ROOT_DIR:\")\n","print(os.listdir(DATA_ROOT_DIR))\n","\n","split_path = os.path.join(DATA_ROOT_DIR, \"resize_224_split_images_1\")\n","print(\"\\nContents of resize_224_split_images_1:\")\n","print(os.listdir(split_path))\n","\n","rgb_path = os.path.join(split_path, \"rgb\")\n","print(\"\\nContents of rgb folder (first 10 files):\")\n","print(os.listdir(rgb_path)[:10])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_zF_dOEjVey2","executionInfo":{"status":"ok","timestamp":1759541370548,"user_tz":300,"elapsed":43,"user":{"displayName":"Aisha Salimgereyeva","userId":"12605674983793144003"}},"outputId":"294dab10-4b40-48d0-cacd-016057151445"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DATA_ROOT_DIR: /content/drive/Shareddrives/Skinterest-2b/data/Resize_Data/\n","\n","Contents of DATA_ROOT_DIR:\n","['.DS_Store', 'resize_224_split_images_6', 'resize_224_split_images_7', 'resize_224_split_images_2', 'resize_224_split_images_3', 'resize_224_split_images_1', 'resize_224_split_images_4', 'resize_224_split_images_5', 'Resize_160', 'resize_224_split_images_8']\n","\n","Contents of resize_224_split_images_1:\n","['.DS_Store', 'rgb', 'hsv', 'lab']\n","\n","Contents of rgb folder (first 10 files):\n","['-1628413175243738425.png', '-1642667040160094612.png', '-1793399078708447490.png', '-2099414475733487459.png', '-1067701776636141108.png', '-1029834132011629215.png', '-2101790873047080311.png', '-1532307918011967845.png', '-1297003684632367094.png', '-144864341463911154.png']\n"]}]},{"cell_type":"code","source":["!pip install tensorflow==2.15\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-s2c_N7OqVWv","executionInfo":{"status":"ok","timestamp":1759538406501,"user_tz":300,"elapsed":1157,"user":{"displayName":"Aisha Salimgereyeva","userId":"12605674983793144003"}},"outputId":"6ad86484-6de8-4435-d379-a7c23eaaf5ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.15 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.19.1, 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.15\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import glob\n","\n","# Collect all rgb images across all split folders\n","all_image_paths = glob.glob(os.path.join(image_root, \"resize_224_split_images_*\", \"rgb\", \"*.png\"))\n","path_map = {os.path.basename(p): p for p in all_image_paths}\n","\n","df['filename'] = df['image_1_path'].apply(lambda x: os.path.basename(str(x)))\n","df['image_path'] = df['filename'].map(path_map)\n","\n","print(df[['image_1_path','filename','image_path']].head())\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ncY-UlVP0zTR","executionInfo":{"status":"ok","timestamp":1759541448545,"user_tz":300,"elapsed":43,"user":{"displayName":"Aisha Salimgereyeva","userId":"12605674983793144003"}},"outputId":"a1154785-0ee7-47d4-b36d-839d0def371a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                              image_1_path                  filename  \\\n","0  dataset/images/-5949315841433628424.png  -5949315841433628424.png   \n","1    dataset/images/325464533153467313.png    325464533153467313.png   \n","2  dataset/images/-6837240536182868524.png  -6837240536182868524.png   \n","3   dataset/images/2983323875335943836.png   2983323875335943836.png   \n","4   dataset/images/3104801012387799539.png   3104801012387799539.png   \n","\n","   image_path  \n","0         NaN  \n","1         NaN  \n","2         NaN  \n","3         NaN  \n","4         NaN  \n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import os, glob\n","\n","# Paths\n","csv_path = '/content/drive/Shareddrives/Skinterest-2b/data/SCIN_CLEAN.csv'\n","image_root = '/content/drive/Shareddrives/Skinterest-2b/data/Resize_Data/'\n","\n","# Load CSV\n","df = pd.read_csv(csv_path)\n","\n","# Flatten all image paths\n","df1 = df[['image_1_path', 'image_1_shot_type']].rename(\n","    columns={'image_1_path': 'partial_path', 'image_1_shot_type': 'label_text'})\n","df2 = df[['image_2_path', 'image_2_shot_type']].rename(\n","    columns={'image_2_path': 'partial_path', 'image_2_shot_type': 'label_text'})\n","df3 = df[['image_3_path', 'image_3_shot_type']].rename(\n","    columns={'image_3_path': 'partial_path', 'image_3_shot_type': 'label_text'})\n","\n","combined_df = pd.concat([df1, df2, df3], ignore_index=True)\n","combined_df.dropna(subset=['partial_path', 'label_text'], inplace=True)\n","\n","# Extract basename\n","combined_df['filename'] = combined_df['partial_path'].apply(lambda x: os.path.basename(str(x)))\n","\n","# Collect all actual images in Drive\n","all_image_paths = glob.glob(os.path.join(image_root, \"resize_224_split_images_*\", \"rgb\", \"*.png\"))\n","\n","# Map basename -> full path\n","path_map = {os.path.basename(p): p for p in all_image_paths}\n","\n","# Attach\n","combined_df['image_path'] = combined_df['filename'].map(path_map)\n","\n","# Drop missing\n","combined_df.dropna(subset=['image_path'], inplace=True)\n","\n","print(\"CSV rows:\", len(df))\n","print(\"Expanded combined_df rows:\", len(combined_df))\n","print(\"Unique filenames matched:\", combined_df['filename'].nunique())\n","print(combined_df.head())\n","\n","print(\"Non-null counts:\")\n","print(\"image_1_path:\", df['image_1_path'].notna().sum())\n","print(\"image_2_path:\", df['image_2_path'].notna().sum())\n","print(\"image_3_path:\", df['image_3_path'].notna().sum())\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UmW1iLDU3mp6","executionInfo":{"status":"ok","timestamp":1759541983031,"user_tz":300,"elapsed":36,"user":{"displayName":"Aisha Salimgereyeva","userId":"12605674983793144003"}},"outputId":"a761ba67-04c8-43f6-f2e1-ee0c1d526c67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CSV rows: 770\n","Expanded combined_df rows: 770\n","Unique filenames matched: 770\n","                              partial_path   label_text  \\\n","0  dataset/images/-5949315841433628424.png  AT_DISTANCE   \n","2  dataset/images/-6837240536182868524.png  AT_DISTANCE   \n","5  dataset/images/-1824718034048585128.png  AT_AN_ANGLE   \n","6  dataset/images/-2805714129362314533.png  AT_AN_ANGLE   \n","9  dataset/images/-4238387650372285498.png     CLOSE_UP   \n","\n","                   filename                                         image_path  \n","0  -5949315841433628424.png  /content/drive/Shareddrives/Skinterest-2b/data...  \n","2  -6837240536182868524.png  /content/drive/Shareddrives/Skinterest-2b/data...  \n","5  -1824718034048585128.png  /content/drive/Shareddrives/Skinterest-2b/data...  \n","6  -2805714129362314533.png  /content/drive/Shareddrives/Skinterest-2b/data...  \n","9  -4238387650372285498.png  /content/drive/Shareddrives/Skinterest-2b/data...  \n","Non-null counts:\n","image_1_path: 770\n","image_2_path: 522\n","image_3_path: 398\n"]}]},{"cell_type":"code","source":["# Full multi-head ResNet152V2 fine-tuning pipeline\n","# Run this after you've already produced combined_df that contains:\n","#   'filename', 'image_path', and CSV columns from SCIN_CLEAN.\n","# Adjust column names in the CONFIG section if your CSV uses different names.\n","\n","import os\n","import math\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras import layers, models, losses, optimizers, callbacks\n","from tensorflow.keras.applications import ResNet152V2\n","from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","# ---------- CONFIG ----------\n","IMAGE_SIZE = (224, 224)\n","BATCH_SIZE = 32\n","AUTO = tf.data.AUTOTUNE\n","EPOCHS_HEADS = 8        # train heads first\n","EPOCHS_UNFREEZE = 12    # then unfreeze top and train more\n","LEARNING_RATE_HEADS = 1e-3\n","LEARNING_RATE_FINETUNE = 5e-5\n","MODEL_INPUT_SHAPE = IMAGE_SIZE + (3,)\n","\n","# Column names in your dataframe (change if different)\n","COL_IMAGE_PATH = 'image_path'   # full path we already produced\n","# For lighting quality (binary): set to column in CSV that indicates harsh lighting.\n","# If none exists, the code will try 'image_?_shot_type' but will fallback to placeholder.\n","COL_LIGHTING_SOURCE = 'image_1_shot_type'  # example; change to real lighting column if available\n","\n","# Undertone column (multi-class). Example candidates: 'monk_skin_tone_label_us' or 'monk_skin_tone_label_india'\n","COL_UNDERTONE = 'monk_skin_tone_label_us'   # change as required\n","\n","# Skin type (Fitzpatrick) column (multi-class)\n","COL_SKIN_TYPE = 'dermatologist_fitzpatrick_skin_type_label_1'  # change as required\n","\n","# Grouping column for evaluation (sensitivity/specificity per group),\n","# typically fitzpatrick or monk skin tone. Set to one of the above or None.\n","EVAL_GROUP_COL = COL_SKIN_TYPE\n","\n","# Loss weights for multi-task loss (you can tune these)\n","LOSS_WEIGHTS = {\n","    'lighting_out': 1.0,\n","    'undertone_out': 1.0,\n","    'skin_type_out': 1.0\n","}\n","\n","# ---------- Helpers & Sanity checks ----------\n","print(\"TensorFlow version:\", tf.__version__)\n","tf.keras.mixed_precision.set_global_policy('mixed_float16')  # speedup on GPU\n","\n","# You must have `combined_df` already in the workspace. If not, load it first:\n","try:\n","    combined_df\n","except NameError:\n","    raise RuntimeError(\"combined_df not found in workspace. Run your preprocessing cell that builds combined_df first.\")\n","\n","print(\"Initial combined_df rows:\", len(combined_df))\n","\n","# Ensure image paths exist — filter out missing files\n","exists_mask = combined_df[COL_IMAGE_PATH].apply(lambda p: os.path.exists(p))\n","if not exists_mask.all():\n","    missing = (~exists_mask).sum()\n","    print(f\"Warning: {missing} entries have missing image file paths; they will be dropped.\")\n","    combined_df = combined_df[exists_mask].copy()\n","\n","# ---------- Create target columns ----------\n","# LIGHTING (binary): try to derive from known shot_type columns, but user should replace with real lighting annotation if available.\n","if COL_LIGHTING_SOURCE in combined_df.columns:\n","    print(f\"Using {COL_LIGHTING_SOURCE} as lighting source to derive 'lighting_label' (HARSH vs NON-HARSH).\")\n","    combined_df['lighting_label'] = combined_df[COL_LIGHTING_SOURCE].astype(str).apply(\n","        lambda x: 1 if 'HARSH' in x.upper() else 0\n","    )\n","else:\n","    print(f\"Note: {COL_LIGHTING_SOURCE} not found. Creating placeholder 'lighting_label' = 0 for all (you should replace with a real column).\")\n","    combined_df['lighting_label'] = 0\n","\n","# UNDERTONE: multi-class — if column missing, fallback to monk_skin_tone_label_us or create placeholder.\n","if COL_UNDERTONE in combined_df.columns and combined_df[COL_UNDERTONE].notna().any():\n","    combined_df['undertone_raw'] = combined_df[COL_UNDERTONE].astype(str)\n","else:\n","    # fallback: try other likely columns\n","    fallbacks = ['monk_skin_tone_label_india', 'monk_skin_tone_label_us']\n","    chosen = None\n","    for col in fallbacks:\n","        if col in combined_df.columns and combined_df[col].notna().any():\n","            chosen = col\n","            break\n","    if chosen:\n","        print(f\"Using fallback undertone column {chosen}\")\n","        combined_df['undertone_raw'] = combined_df[chosen].astype(str)\n","    else:\n","        print(\"No undertone column found. Creating placeholder 'undertone_raw' with single class 'unknown'.\")\n","        combined_df['undertone_raw'] = 'unknown'\n","\n","# SKIN TYPE (Fitzpatrick multi-class): try provided column; fallback if missing\n","if COL_SKIN_TYPE in combined_df.columns and combined_df[COL_SKIN_TYPE].notna().any():\n","    combined_df['skin_type_raw'] = combined_df[COL_SKIN_TYPE].astype(str)\n","else:\n","    # try dermatologist_fitzpatrick_skin_type_label_2/3\n","    for col in ['dermatologist_fitzpatrick_skin_type_label_2', 'dermatologist_fitzpatrick_skin_type_label_3']:\n","        if col in combined_df.columns and combined_df[col].notna().any():\n","            combined_df['skin_type_raw'] = combined_df[col].astype(str)\n","            print(f\"Using fallback skin type column {col}\")\n","            break\n","    else:\n","        print(\"No Fitzpatrick column found. Creating placeholder 'skin_type_raw' with single class 'unknown'.\")\n","        combined_df['skin_type_raw'] = 'unknown'\n","\n","# Encode categorical targets\n","le_undertone = LabelEncoder()\n","combined_df['undertone_id'] = le_undertone.fit_transform(combined_df['undertone_raw'].fillna('unknown'))\n","\n","le_skin = LabelEncoder()\n","combined_df['skin_type_id'] = le_skin.fit_transform(combined_df['skin_type_raw'].fillna('unknown'))\n","\n","print(\"Undertone classes:\", le_undertone.classes_)\n","print(\"Skin-type classes:\", le_skin.classes_)\n","print(\"Lighting positive count (HARSH=1):\", combined_df['lighting_label'].sum(), \"/\", len(combined_df))\n","\n","\n","print(combined_df['lighting_label'].value_counts())\n","print(combined_df['undertone_raw'].value_counts())\n","print(combined_df['skin_type_raw'].value_counts())\n","\n","\n","# ---------- Train/Val/Test split ----------\n","from sklearn.model_selection import train_test_split\n","final_df = combined_df.copy()\n","train_df, test_val_df = train_test_split(final_df, test_size=0.3, random_state=42, stratify=final_df['lighting_label'])\n","val_df, test_df = train_test_split(test_val_df, test_size=0.5, random_state=42, stratify=test_val_df['lighting_label'])\n","\n","print(\"Split sizes:\", len(train_df), len(val_df), len(test_df))\n","\n","# ---------- Build tf.data pipelines ----------\n","def decode_and_preprocess(path, label_lighting, label_undertone, label_skin):\n","    # read\n","    image = tf.io.read_file(path)\n","    image = tf.image.decode_image(image, channels=3)  # handles png/jpg\n","    image.set_shape([None, None, 3])\n","    # convert to float32\n","    image = tf.image.convert_image_dtype(image, tf.float32)\n","    # resize\n","    image = tf.image.resize(image, IMAGE_SIZE)\n","    # preprocessing for ResNetV2: scale to [-1,1]\n","    image = resnet_preprocess(image * 255.0)  # resnet preprocess expects [0,255] input\n","    return image, {'lighting_out': label_lighting, 'undertone_out': label_undertone, 'skin_type_out': label_skin}\n","\n","# Data augmentation (applied only to training)\n","data_augmentation = tf.keras.Sequential([\n","    layers.RandomFlip(\"horizontal\"),\n","    layers.RandomRotation(0.08),\n","    layers.RandomZoom(0.08),\n","    # Add more if needed\n","], name=\"data_augmentation\")\n","\n","def preprocess_for_train(path, lighting, undertone, skin):\n","    image, labels = decode_and_preprocess(path, lighting, undertone, skin)\n","    image = data_augmentation(image)\n","    return image, labels\n","\n","def preprocess_for_eval(path, lighting, undertone, skin):\n","    image, labels = decode_and_preprocess(path, lighting, undertone, skin)\n","    return image, labels\n","\n","def make_dataset(df, shuffle=False, batch_size=BATCH_SIZE, training=False):\n","    paths = df[COL_IMAGE_PATH].values\n","    lighting = df['lighting_label'].astype(np.int32).values\n","    undertone = df['undertone_id'].astype(np.int32).values\n","    skin = df['skin_type_id'].astype(np.int32).values\n","\n","    ds = tf.data.Dataset.from_tensor_slices((paths, lighting, undertone, skin))\n","    if shuffle:\n","        ds = ds.shuffle(buffer_size=len(paths))\n","    map_fn = preprocess_for_train if training else preprocess_for_eval\n","    ds = ds.map(map_fn, num_parallel_calls=AUTO)\n","    ds = ds.batch(batch_size)\n","    ds = ds.prefetch(AUTO)\n","    return ds\n","\n","train_ds = make_dataset(train_df, shuffle=True, training=True)\n","val_ds = make_dataset(val_df, shuffle=False, training=False)\n","test_ds = make_dataset(test_df, shuffle=False, training=False)\n","\n","# ---------- Compute class weights for lighting (binary) to help imbalance ----------\n","classes = np.unique(train_df['lighting_label'])\n","class_weights = {}\n","if len(classes) > 1:\n","    weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df['lighting_label'].values)\n","    class_weights = {int(c): float(w) for c, w in zip(classes, weights)}\n","    print(\"Lighting class weights:\", class_weights)\n","else:\n","    print(\"Only one class present for lighting in train set; class weighting skipped.\")\n","    class_weights = None\n","\n","# ---------- Build the multi-head model ----------\n","def build_model(num_undertone_classes, num_skin_classes, input_shape=MODEL_INPUT_SHAPE):\n","    # Backbone without top\n","    base = ResNet152V2(include_top=False, input_shape=input_shape, weights='imagenet', pooling='avg')\n","    base.trainable = False  # freeze initially\n","\n","    inputs = layers.Input(shape=input_shape)\n","    x = inputs\n","    # base preprocessing already applied in pipeline\n","    x = base(x, training=False)\n","    x = layers.Dropout(0.3)(x)\n","\n","    # Lighting head (binary)\n","    lighting = layers.Dense(128, activation='relu')(x)\n","    lighting = layers.Dropout(0.2)(lighting)\n","    lighting_out = layers.Dense(1, activation='sigmoid', dtype='float32', name='lighting_out')(lighting)\n","\n","    # Undertone head (multiclass)\n","    ud = layers.Dense(128, activation='relu')(x)\n","    ud = layers.Dropout(0.2)(ud)\n","    undertone_out = layers.Dense(num_undertone_classes, activation='softmax', dtype='float32', name='undertone_out')(ud)\n","\n","    # Skin-type head (multiclass)\n","    st = layers.Dense(128, activation='relu')(x)\n","    st = layers.Dropout(0.2)(st)\n","    skin_type_out = layers.Dense(num_skin_classes, activation='softmax', dtype='float32', name='skin_type_out')(st)\n","\n","    model = models.Model(inputs=inputs, outputs=[lighting_out, undertone_out, skin_type_out])\n","    return model\n","\n","num_undertone_classes = len(le_undertone.classes_)\n","num_skin_classes = len(le_skin.classes_)\n","model = build_model(num_undertone_classes, num_skin_classes)\n","model.summary()\n","\n","# ---------- Compile for head training ----------\n","losses_dict = {\n","    'lighting_out': losses.BinaryCrossentropy(),\n","    'undertone_out': losses.SparseCategoricalCrossentropy(),\n","    'skin_type_out': losses.SparseCategoricalCrossentropy()\n","}\n","\n","metrics_dict = {\n","    'lighting_out': [tf.keras.metrics.BinaryAccuracy(name='acc'), tf.keras.metrics.AUC(name='auc')],\n","    'undertone_out': [tf.keras.metrics.SparseCategoricalAccuracy(name='acc')],\n","    'skin_type_out': [tf.keras.metrics.SparseCategoricalAccuracy(name='acc')]\n","}\n","\n","opt = optimizers.Adam(learning_rate=LEARNING_RATE_HEADS)\n","model.compile(optimizer=opt, loss=losses_dict, loss_weights=LOSS_WEIGHTS, metrics=metrics_dict)\n","\n","# ---------- Callbacks ----------\n","ckpt_path = \"/content/drive/MyDrive/skinterest_resnet_multitask_ckpt.weights.h5\"\n","\n","cb = [\n","    callbacks.ModelCheckpoint(\n","        ckpt_path,\n","        monitor='val_lighting_out_acc',\n","        save_best_only=True,\n","        save_weights_only=True,\n","        mode='max'\n","    ),\n","    callbacks.EarlyStopping(\n","        monitor='val_lighting_out_acc',\n","        patience=5,\n","        restore_best_weights=True,\n","        mode='max'\n","    ),\n","    callbacks.ReduceLROnPlateau(\n","        monitor='val_lighting_out_acc',\n","        factor=0.5,\n","        patience=3,\n","        mode='max'\n","    )\n","]\n","\n","\n","\n","# ---------- Train heads ----------\n","history_heads = model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=EPOCHS_HEADS,\n","    callbacks=cb,\n","    # Keras does class_weight only supports single-output mapping. We'll handle lighting class imbalance using sample weighting trick if needed.\n",")\n","\n","# ---------- Unfreeze top of backbone for fine-tuning ----------\n","# Unfreeze last N layers of base\n","# ---------- Unfreeze top of backbone for fine-tuning ----------\n","base = model.get_layer(\"resnet152v2\")  # <- directly fetch backbone by name\n","\n","if base is None:\n","    print(\"Warning: could not find backbone to unfreeze; skipping fine-tuning.\")\n","else:\n","    base.trainable = True\n","    # Freeze earlier layers, unfreeze last N\n","    fine_tune_at = int(len(base.layers) * 0.6)  # unfreeze top 40%\n","    for i, layer in enumerate(base.layers):\n","        layer.trainable = i >= fine_tune_at\n","\n","    # Recompile with lower LR\n","    opt_finetune = optimizers.Adam(learning_rate=LEARNING_RATE_FINETUNE)\n","    model.compile(optimizer=opt_finetune, loss=losses_dict, loss_weights=LOSS_WEIGHTS, metrics=metrics_dict)\n","\n","    history_ft = model.fit(\n","        train_ds,\n","        validation_data=val_ds,\n","        epochs=EPOCHS_UNFREEZE,\n","        callbacks=cb\n","    )\n","\n","# ---------- Evaluation on test set ----------\n","print(\"Evaluating on test set...\")\n","test_pred = model.predict(test_ds, verbose=1)\n","\n","# test_pred is list: [lighting_probs, undertone_probs, skin_probs]\n","lighting_probs = np.asarray(test_pred[0]).squeeze()\n","undertone_probs = np.asarray(test_pred[1])\n","skin_probs = np.asarray(test_pred[2])\n","\n","lighting_preds = (lighting_probs >= 0.5).astype(int)\n","# convert undertone and skin preds to labels\n","undertone_preds = np.argmax(undertone_probs, axis=1)\n","skin_preds = np.argmax(skin_probs, axis=1)\n","\n","# Ground truth arrays\n","y_lighting = test_df['lighting_label'].astype(int).values\n","y_undertone = test_df['undertone_id'].astype(int).values\n","y_skin = test_df['skin_type_id'].astype(int).values\n","\n","# Overall metrics\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","print(\"Lighting metrics (binary):\")\n","print(\"Accuracy:\", accuracy_score(y_lighting, lighting_preds))\n","print(\"Precision:\", precision_score(y_lighting, lighting_preds, zero_division=0))\n","print(\"Recall (sensitivity):\", recall_score(y_lighting, lighting_preds, zero_division=0))\n","print(\"Specificity:\", None if len(np.unique(y_lighting))<2 else \\\n","      (confusion_matrix(y_lighting, lighting_preds)[0,0] / (confusion_matrix(y_lighting, lighting_preds)[0,0] + confusion_matrix(y_lighting, lighting_preds)[0,1])))\n","print(\"F1:\", f1_score(y_lighting, lighting_preds, zero_division=0))\n","\n","print(\"\\nUndertone classification report:\")\n","print(classification_report(y_undertone, undertone_preds, target_names=le_undertone.classes_, zero_division=0))\n","\n","print(\"\\nSkin-type classification report:\")\n","print(classification_report(y_skin, skin_preds, target_names=le_skin.classes_, zero_division=0))\n","\n","# ---------- Sensitivity/Specificity by group ----------\n","def sens_spec_by_group(y_true_binary, y_pred_binary, group_values, group_name):\n","    df_eval = pd.DataFrame({'y_true': y_true_binary, 'y_pred': y_pred_binary, 'group': group_values})\n","    groups = df_eval['group'].unique()\n","    rows = []\n","    for g in groups:\n","        sub = df_eval[df_eval['group'] == g]\n","        if len(sub) == 0:\n","            continue\n","        cm = confusion_matrix(sub['y_true'], sub['y_pred'], labels=[0,1])\n","        tn, fp, fn, tp = (cm.ravel() if cm.size==4 else (cm[0,0], 0, 0, cm[0,0]))\n","        sens = tp / (tp + fn) if (tp + fn) > 0 else None\n","        spec = tn / (tn + fp) if (tn + fp) > 0 else None\n","        rows.append((g, len(sub), sens, spec, tp, fn, tn, fp))\n","    rows = sorted(rows, key=lambda r: r[1], reverse=True)\n","    print(f\"\\nSensitivity/Specificity by {group_name}:\")\n","    print(\"group | n | sensitivity | specificity | tp | fn | tn | fp\")\n","    for r in rows:\n","        print(r)\n","\n","# If EVAL_GROUP_COL exists in test_df, compute sensitivity/specificity for lighting across groups\n","if EVAL_GROUP_COL in test_df.columns:\n","    group_vals = test_df[EVAL_GROUP_COL].astype(str).values\n","    sens_spec_by_group(y_lighting, lighting_preds, group_vals, EVAL_GROUP_COL)\n","else:\n","    print(f\"Eval group column {EVAL_GROUP_COL} not present in test_df; skipping per-group sensitivity/specificity.\")\n","\n","# Save final model weights (optional)\n","final_weights_path = \"/content/drive/MyDrive/skinterest_multitask_final.h5\"\n","model.save_weights(final_weights_path)\n","print(\"Saved model weights to:\", final_weights_path)\n","\n","# ---------- End ----------\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"OagHSLdtloz5","executionInfo":{"status":"error","timestamp":1759541892115,"user_tz":300,"elapsed":425486,"user":{"displayName":"Aisha Salimgereyeva","userId":"12605674983793144003"}},"outputId":"ec7e2bae-bcf3-45ed-8ca2-decad3c38cdf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.20.0\n","Initial combined_df rows: 770\n","Note: image_1_shot_type not found. Creating placeholder 'lighting_label' = 0 for all (you should replace with a real column).\n","No undertone column found. Creating placeholder 'undertone_raw' with single class 'unknown'.\n","No Fitzpatrick column found. Creating placeholder 'skin_type_raw' with single class 'unknown'.\n","Undertone classes: ['unknown']\n","Skin-type classes: ['unknown']\n","Lighting positive count (HARSH=1): 0 / 770\n","lighting_label\n","0    770\n","Name: count, dtype: int64\n","undertone_raw\n","unknown    770\n","Name: count, dtype: int64\n","skin_type_raw\n","unknown    770\n","Name: count, dtype: int64\n","Split sizes: 539 115 116\n","Only one class present for lighting in train set; class weighting skipped.\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ resnet152v2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │ \u001b[38;5;34m58,331,648\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n","│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ resnet152v2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m262,272\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m262,272\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m262,272\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lighting_out        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ undertone_out       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ skin_type_out       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ resnet152v2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">58,331,648</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ resnet152v2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lighting_out        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ undertone_out       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ skin_type_out       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m59,118,851\u001b[0m (225.52 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,118,851</span> (225.52 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m787,203\u001b[0m (3.00 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">787,203</span> (3.00 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m58,331,648\u001b[0m (222.52 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,331,648</span> (222.52 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/8\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/ops/nn.py:944: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 2s/step - lighting_out_acc: 0.9028 - lighting_out_auc: 0.0000e+00 - lighting_out_loss: 0.1557 - loss: 0.1558 - skin_type_out_acc: 1.0000 - skin_type_out_loss: 0.0000e+00 - undertone_out_acc: 1.0000 - undertone_out_loss: 0.0000e+00 - val_lighting_out_acc: 1.0000 - val_lighting_out_auc: 0.0000e+00 - val_lighting_out_loss: 7.7138e-07 - val_loss: 8.4831e-07 - val_skin_type_out_acc: 1.0000 - val_skin_type_out_loss: 0.0000e+00 - val_undertone_out_acc: 1.0000 - val_undertone_out_loss: 0.0000e+00 - learning_rate: 0.0010\n","Epoch 2/8\n","\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - lighting_out_acc: 1.0000 - lighting_out_auc: 0.0000e+00 - lighting_out_loss: 1.1205e-06 - loss: 1.1214e-06 - skin_type_out_acc: 1.0000 - skin_type_out_loss: 0.0000e+00 - undertone_out_acc: 1.0000 - undertone_out_loss: 0.0000e+00 - val_lighting_out_acc: 1.0000 - val_lighting_out_auc: 0.0000e+00 - val_lighting_out_loss: 1.4356e-07 - val_loss: 1.5871e-07 - val_skin_type_out_acc: 1.0000 - val_skin_type_out_loss: 0.0000e+00 - val_undertone_out_acc: 1.0000 - val_undertone_out_loss: 0.0000e+00 - learning_rate: 0.0010\n","Epoch 3/8\n","\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - lighting_out_acc: 1.0000 - lighting_out_auc: 0.0000e+00 - lighting_out_loss: 1.8933e-07 - loss: 1.8869e-07 - skin_type_out_acc: 1.0000 - skin_type_out_loss: 0.0000e+00 - undertone_out_acc: 1.0000 - undertone_out_loss: 0.0000e+00 - val_lighting_out_acc: 1.0000 - val_lighting_out_auc: 0.0000e+00 - val_lighting_out_loss: 1.0175e-07 - val_loss: 1.1258e-07 - val_skin_type_out_acc: 1.0000 - val_skin_type_out_loss: 0.0000e+00 - val_undertone_out_acc: 1.0000 - val_undertone_out_loss: 0.0000e+00 - learning_rate: 0.0010\n","Epoch 4/8\n","\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - lighting_out_acc: 1.0000 - lighting_out_auc: 0.0000e+00 - lighting_out_loss: 1.7950e-07 - loss: 1.7962e-07 - skin_type_out_acc: 1.0000 - skin_type_out_loss: 0.0000e+00 - undertone_out_acc: 1.0000 - undertone_out_loss: 0.0000e+00 - val_lighting_out_acc: 1.0000 - val_lighting_out_auc: 0.0000e+00 - val_lighting_out_loss: 9.4992e-08 - val_loss: 1.0511e-07 - val_skin_type_out_acc: 1.0000 - val_skin_type_out_loss: 0.0000e+00 - val_undertone_out_acc: 1.0000 - val_undertone_out_loss: 0.0000e+00 - learning_rate: 0.0010\n","Epoch 5/8\n","\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - lighting_out_acc: 1.0000 - lighting_out_auc: 0.0000e+00 - lighting_out_loss: 3.8645e-07 - loss: 3.8690e-07 - skin_type_out_acc: 1.0000 - skin_type_out_loss: 0.0000e+00 - undertone_out_acc: 1.0000 - undertone_out_loss: 0.0000e+00 - val_lighting_out_acc: 1.0000 - val_lighting_out_auc: 0.0000e+00 - val_lighting_out_loss: 9.4103e-08 - val_loss: 1.0413e-07 - val_skin_type_out_acc: 1.0000 - val_skin_type_out_loss: 0.0000e+00 - val_undertone_out_acc: 1.0000 - val_undertone_out_loss: 0.0000e+00 - learning_rate: 5.0000e-04\n","Epoch 6/8\n","\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - lighting_out_acc: 1.0000 - lighting_out_auc: 0.0000e+00 - lighting_out_loss: 2.6886e-07 - loss: 2.6870e-07 - skin_type_out_acc: 1.0000 - skin_type_out_loss: 0.0000e+00 - undertone_out_acc: 1.0000 - undertone_out_loss: 0.0000e+00 - val_lighting_out_acc: 1.0000 - val_lighting_out_auc: 0.0000e+00 - val_lighting_out_loss: 9.3711e-08 - val_loss: 1.0369e-07 - val_skin_type_out_acc: 1.0000 - val_skin_type_out_loss: 0.0000e+00 - val_undertone_out_acc: 1.0000 - val_undertone_out_loss: 0.0000e+00 - learning_rate: 5.0000e-04\n","Epoch 1/12\n","\u001b[1m 1/17\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33:16\u001b[0m 125s/step - lighting_out_acc: 1.0000 - lighting_out_auc: 0.0000e+00 - lighting_out_loss: 1.8426e-04 - loss: 1.8426e-04 - skin_type_out_acc: 1.0000 - skin_type_out_loss: 0.0000e+00 - undertone_out_acc: 1.0000 - undertone_out_loss: 0.0000e+00"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3317044716.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_finetune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOSS_WEIGHTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     history_ft = model.fit(\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}